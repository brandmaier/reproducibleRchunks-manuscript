---
title             : "`reprodubileRchunks`: R Markdown Code Chunks with Automated Reproducibility Tests"
shorttitle        : "Title"

author: 
  - name          : "Andreas M. Brandmaier"
    affiliation   : "1,2,3"
    corresponding : no    # Define only one corresponding author
    address       : "Rüdesheimer Str. 50, 14197 Berlin"
    email         : "andreas.brandmaier@medicalschool-berlin.de"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"

  - name          : "Aaron Peikert"
    affiliation   : "2,3"
    corresponding : no    # Define only one corresponding author
    email         : "peikert@mpib-berlin.mpg.de"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Writing - Review & Editing"
            
affiliation:
  - id            : "1"
    institution   : "Department of Psychology, MSB Medical School Berlin"
  - id            : "2"
    institution   : "Center for Lifespan Psychology, Max Planck Institute for Human Development"
  - id            : "3"
    institution   : "Max Planck UCL Centre for Computational Psychiatry and Ageing Research"

authornote: |
  Please address correspondence to: Prof. Dr. Andreas M. Brandmaier, Rüdesheimerstr. 50, 14197 Berlin, Germany. Email: andreas.brandmaier@medicalschool-berlin.de

abstract: |
 Computational results are considered _reproducible_ if the same computation on the same data yields the same results if performed on a different computer or on the same computer later in time. Reproducibility is a prerequisite for replicable, robust and transparent research in digital environments.
 Various approaches have been suggested to increase chances of reproducibility.
 Many of them rely on R Markdown as a language to dynamically generate reproducible research assets (e.g., reports, posters, presentation).
 However, a simple way to test whether different parts of these assets are reproducible is still missing.
 We introduce the R package `reproducibleRchunks`, which provides a new type of code chunk in R Markdown documents, which automatically store meta data about original computational results and check later reproductions of the research assets for reproduction. 
 With a minimal change to users' workflows, we hope that this approach increases transparency and trustworthiness of research assets.
  
keywords          : "reproducibility, Markdown, R"
wordcount         : "`r tryCatch(wordcountaddin::word_count(here::here('manuscript.Rmd')))`"

bibliography      : "r-references.bib"

floatsintext      : yes
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man" #man
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library("reproducibleRchunks")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Computational results, including those from statistical data analyses, are considered _reproducible_ if the same computation on the same data yields the same results if performed on a different computer or on the same computer later in time. Reproducibility is a prerequisite for replicable, robust and transparent research in digital environments.
 Various approaches have been suggested to increase chances of reproducibility [@peikert2021reproducible, @renv2022, @chan2023rang, @nagraj2023pracpac].
 Many approaches rely on R Markdown.
R Markdown is a simple language, which allows mixing natural language, simple formatting instructions (e.g., what is a headline, what should appear in bold face, what are list elements) and computer code (R, python or others) in a single document [@xie2018r].
When ever the document is compiled (or, _knitted_ in R speak), the computer code chunks are executed and allow for the dynamic generation of parts of the document.
A particular advantage of R Markdown is that it helps avoiding common threats to reproducibility of code, first and foremost, copy&paste errors between results of a regular R script and a scientific report (e.g., a Word document) [@peikert2021reproducible];
further, it is suitable to generate a variety of scientific assets such as posters [@posterdown2019], presentations [@xaringan2022], resumés [@vitae2023] or preregistrations [@peikert2021reproducible] from the same Markdown language.


The `reproducibleRchunk` package allows users to make computational results in R Markdown documents automatically testable for reproduction (does the same script with the same data produce the same results, e.g. on a different computer and/or later in time) with minimal changes to your workflow.
There is only a single thing users need to change in their Markdown document if they are already using R Markdown: In the first code chunk of their document, load the package and then change the code chunk type from `r` to `reproducibleR` for all code chunks that should be automatically checked for reproducibility.
In `reproducibleR` chunks, all newly declared variables are automatically identified and their contents are stored in a meta data file. 
Furthermore, the code chunk itself (that is, the code syntax) is also fingerprinted and stored, such that later non-reproduction can be traced back to either changes of the code chunk or changes of the computation.
Once a document is reproduced, in R-Markdown-speak _knitted_ again [@yihui2015], all computational results generated in the `reproducibleR` chunks are automatically tested for reproducibility.
The result of the reproduction attempt is displayed in a reproducibility report for each chunk.
Customizations of these reports are possible as we will demonstrate later.
A simple way to assess reproducibility of computational results would be  a bitwise comparison of the entire knitted document (i.e., the resulting HTML, PDF or DOCX file).
Yet, a particular advantage of using `reproducibleR` chunks is that users have a fine-grained control over which stages of a data analysis or any other computation should be checked for reproducibility. 
For example, if there is non-reproducibility, is it due to changes in the data file, during data cleaning or statistical modeling. 
Using `reproducibleRchunks`, users can separately assess the reproducibility of each stage of a given data analysis (or any other computation).
Also, not all elements of a research asset are required to reproduce (such as the current date displayed in a presentation of results).
In the following, we will briefly demonstrate how successful and failed reproduction will be checked and displayed by the package, we will go into details of the mechanics of the package, and discuss potential cases of non-reproducibility and how the package will handle these.

# Demonstration

The core functionality of the package is to provide a `reproducibleR` chunk, which are code chunks in R Markdown documents that can be used like regular `R`-chunks (including name labels and the usual options regulating output parameters). 
The package should be loaded in the first regular `R` code chunk in the document using
```{r eval=FALSE, echo=TRUE}
library(reproducibleRchunks)
```
On load, the package registers a new type of code chunk called `reproducibleR`.
The following example shows a snippet from an R Markdown file, in which there is a `reproducibleR` chunk with the name _addition_. 
In the chunk, a new variable `my_sum` is declared and defined to be the sum of a variable `x` plus one. 
Variable `x` was declared in an earlier code chunk (not shown in the following screenshot) and is thus not subject to the reproducibility test of this chunk. 

```{r rstudio-screen1, fig.cap=""}
knitr::include_graphics("img/rstudio-screenshot-marker2.png")
```

In this example, the R Markdown document is knitted into a HTML-report.
On document generation, the R code in the `reproducibleR`-chunk is executed and the value of `my_sum` is stored in a separate file. 
A reproducibility report is shown below the code chunk to indicate all variables of the given chunk whose data are stored. The following is a snippet from the HTML generated on the first ever creation of the document:

```{r rstudio-screen2, fig.cap="Snippet from a knitted Markdown file showing the reproducibility report when a document is generated for the first time."}
knitr::include_graphics("img/generation-step1.png")
```
Once the document is generated a second time and the reproducibility data exists, the computation is executed anew and its results are compared to the stored reproducibility information on a variable by variable basis.
The reproducibility report will now contain information about the success of the individual reproduction attempts.
In our example, the computation was reproduced successfully:

```{r rstudio-screen3, fig.cap="Snippet from a knitted Markdown file showing the reproducibility report when a document is re-generated and a variable is successfully reproduced."}
knitr::include_graphics("img/generation-step2.png")

```
If the document is regenerated and the computational result is not identical to the original result, this failure of reproduction is indicated in the reproducibility report.
In this example, if someone changed the value of `x`, variable `my_sum` would change and the following failure would be displayed:

```{r rstudio-screen4, fig.cap="Snippet from a knitted Markdown file showing the reproducibility report when a document is re-generated and the reproduction of a variable failed."}
knitr::include_graphics("img/generation-step2-failed.png")

```

# Methods

In the following, we will describe how the package stores, retrieves and compares reproducibility information.
First of all, the package executes `reproducibleR` code chunks just as regular R code chunks.
That is, there are no additional limits imposed on what can be computed and tested for reproducibility.
Once code execution is completed, the package gathers information about all variables that were newly declared in the current chunk. 
The contents of those variables are stored in a separate JSON data file.
JSON is a shorthand for Java Script Object Notation and its main features are that it is an open standard for mapping complex objects to text files with high simplicity and readability for machines and humans [@lennon2009introduction].
The name of the JSON file contains the original Markdown file and the chunk label and always starts with the prefix `repro`). 
That is, if reproducibility information of a chunk labelled `datacleaning` in the file `sem_analysis.Rmd` is stored in a file called _.repro_sem_analysis.Rmd_datacleaning.json_.
Once the document is re-generated and JSON data files exist, their content is checked against the newly computed chunk variables for identity.

```{r echo=FALSE, eval=TRUE}
set.seed(42)
numbers_ex <- sample(1:10, 5)
```
Here is an example of how the contents of a single variable called `numbers` is stored in JSON format. Here, the variable content is a vector of five random draws from the number one to ten: `r numbers_ex` generated by the following command:

```{r echo=TRUE, eval=FALSE}
numbers_ex <- sample(1:10, 5)
```

This vector can be serialized in raw format to a JSON format as follows:

```{r echo=FALSE, eval=TRUE, results="asis"}
content <- jsonlite::serializeJSON(numbers_ex, pretty=TRUE)
content <- gsub(pattern="\\n",replacement="  \n", x=content)
cat(content)
```

For privacy reasons and to save disk space, we actually do not store the raw data by default but only fingerprints of the data.
Fingerprints are realized via so-called one-way hash functions, which can take an arbitrary large digital object and map it onto a fixed-size object, which is typically displayed as hexadecimal string.
Fingerprints allow for an efficient comparison of identity of results while avoid data security and privacy issues as they do not allow to reproduce the original data.
The following shows how the information about variable `numbers` is stored using a SHA256-fingerprint:

```{r echo=FALSE, eval=TRUE, results="asis"}
content <- jsonlite::serializeJSON(digest::digest(numbers_ex,algo = "sha256"),pretty=TRUE)
content <- gsub(pattern="\\n",replacement="  \n", x=content)
cat(content)
```

```{r jsonschema, fig.cap="(ref:jsonschema) A JSON file stores fingerprints of all newly declared variables in a reproducibleR-chunk as well as fingerprints of the code chunk syntax."}
knitr::include_graphics("img/schema-json-fingerprints.png")

```

In addition to the contents of all newly declared variables in a `reproducibleR`-chunk, the package also stores fingerprints of the code syntax.
This is shown in the schematic in Figure \@ref(jsonschema).


## Comparisons of computational results

Once a document is (re-)generated with reproducibility data present (i.e., reproducibleRchunks with matching JSON files), the package will compare all objects amenable for reproducibility checks.
Identity of original and reproduced results are checked with `identical()` function from R's `base` package, which according to the R documentation is a "safe and reliable way to test two objects (even complex ones) for being exactly identical" (see `?identical` R documentation).
Using exact comparisons of object identity is sometimes more restrictive than one would expect.
This is particularly true for numeric comparisons. 
For example results of numeric computations may be logically equivalent but not bitwise equivalent; in numeric representations, there is sometimes a bit reserved for the sign of a number, thus it can be possible to have -0 and +0, which both are exactly zero in a decimal representation but the binary representations differ.
Numeric representations may also differ up to a given numeric precision, for example, when comparing reproducibility across machines that work with different numerical precision (32bit vs 64bit). To avoid such problems, `reproducibleRchunks` rounds numeric values to a given precision (by default, eight digits)


### Types of objects

What types of objects are suited for automated reproducibility tests? The answer is simple. In principle, any R object is suitable no matter how complex.
For creating fingerprints, the package uses R's serialization methods, which take any R object as input.
That is, all of the following variables (`x` of class _integer_, `y` of class _character_, and `z` of class _lm_) are valid examples for the automated reproducibility checks:

```{r examples, eval=FALSE, echo=TRUE}
  x <- 1:10
  y <- "qr"
  z <- lm(x~1, method=y)
```

## Tutorial

In the following, we provide a practical guide on how to use reproducible R code chunks in Markdown documents.

First, we define a reproducible code chunk by setting the code chunk language to `reproducibleR`. 
Once this document is rendered the first time, a data file will be created, which stores all reproducible results, which are computed in this code block. 
The name of the file will contain the label name (in this example `abc`). 
The following code block contains two reproducible results stored in variables `x` and `y`. 
Those computations can be based on results computed in previous chunks (here, variable `e`).
If the R markdown document is rendered a second time, the computational results are recomputed and compared against the original results. 
For each result, success or failure or reproduction is reported separately.
```{r eval=TRUE,echo=FALSE}
e<-1
```

```{reproducibleR abc,echo=TRUE,eval=TRUE}
xu <- rnorm(10, mean=0, sd=1) + e
y <- 4 * 4
```

If nothing goes wrong, rendering this markdown twice should result in a message of successful replication. 
To break reproducibility, remove or comment out the `set.seed()` command and render the document again. 
Now, you should obtain an error message indicating that the result of `x` could not be reproduced while `y` still reproduces as it is not dependend on the random number generator.

In practice, we expect cases, in which the reproduction report should be displayed (e.g., when testing and reproducing) or not (e.g., when rendering a presentation or a manuscript for submission to a journal). 
By default, reproduction report statements are produced for each chunk. 
Displaying these statements can be suppressed on a chunk-by-chunk basis by setting the chunk option 


### Changing defaults

Some default behavior of the package can be changed via R `options()`. The package defines the following options:

- reproducibleRchunks.digits: This is the number of digits for rounding of numbers and controls the numeric precision of the reproducibility checks. By default, this is $8$.
- reproducibleRchunks.filetype: By default this is 'json' 
- reproducibleRchunks.hashing:
- reproducibleRchunks.hashing_algorithm: This is the hashing algorithm, which is used to generate fingerprints of variable contents. By default, the `digest` package is used
- reproducibleRchunks.templates:

Here are a few examples how these options can be changed.

First, it is possible to change the precision with which numeric results are stored. By default, this is up to 8 digits. The following line reduces the precision to only four digits after the decimal point:

```{r eval=FALSE, echo=TRUE}
options(reproducibleRchunks.digits = 4)
```

By default, computational results are stored as fingerprints using a hash function. 
To store data in raw format, switch off hashing with the following option:

```{r eval=FALSE, echo=TRUE}
options(reproducibleRchunks.hashing = FALSE)
```

There are various hashing functions available, which have different features. Generally, hashing functions should map objects of arbitrary size to a fixed-size alphanumeric string, they should be efficient to compute and minimize the chance of collision, that is, the chance that to different objects are mapped on to the same fingerprint.
Supported algorithm (through the digest package, @digest2022) are sha1, crc32, sha256, sha512, xxhash32, xxhash64, murmur32, spookyhash and blake3. 
To reduce chance of collisions, the package defaults to `sha256` which results in fingerprints of 64 hexadecimal characters, which corresponds to a 256-bit fingerprint.

```{r eval=FALSE, echo=TRUE}
options(reproducibleRchunks.hashing_algorithm = "sha256")
```

Note that these options can be chosen differently for each chunk.
That is, it is possible, to use fingerprints for storing results of one chunk and plain data storage for results of another chunk.

### Customization

Last, users of this package can customize the appearance of the reproducibility reports. 
This can be done by either using convenient template layouts, which can be tailored to each specific (pandoc) output format, that is, for example a template for HTML file and a different LaTeX template for PDF files. 
Or, users can obtain a summary of the reproduction status of every variable in every chunk. 
This information is returned by function `get_reproducibility_summary()` which returns a `data.frame` with three columns. 
The first column contains the name of the code chunk, the second column contains a variable name and the third column contains a boolean variable representing whether the reproduction was a success. 
This information can be used to either generate custom reports, such as one general report at the very end of the document, or write reports to a separate file.
The alternative is to just use the default reports that are appended to each code chunk output (as shown in the previous examples). 
Here, users cannot change the content but have some degrees of freedom in styling the output.
The option `reproducibleRchunks.templates` stores the default templates that are used for displaying the report information. 
It is a list of key-value pairs where the key is the final pandoc output format (typically either html, pdf or docx) and the value is a string containing formatting information.
If html output is chosen, it can contain any valid HTML/CSS code, if the output is pdf, it can contain any valid LaTeX code.
These formatting instructions can contain two placeholders, which the package will replace with the title of the report (\${title}) and the content of the report (\${content}).

```{r echo=TRUE, eval=FALSE}
    options(reproducibleRchunks.templates = list(
      html="<div style='border: 3px solid black; 
      padding: 10px 10px 10px 10px; 
      background-color: #EEEEEE;'>
      <h5>${title}</h5>${content}</div>"))
```

For example, the following code reformats the appearance of the code report in PDF documents, such that the report is enclosed by two horizontal lines (\\hr elements), the title is displayed as a section header, there is a medium skip between title and content and content is displayed in a smaller font size:

```{r echo=TRUE, eval=FALSE}
    options(reproducibleRchunks.templates = list(
      pdf="\\hr \\section{${title}} 
      \\medskip \\smaller 
      ${content} \\hr"
    )
```

If users wish to entirely suppress the default reproducibility reports, they can use the option `report=FALSE` as part of the code chunk options to suppress the display. 
Note that reproduction is still attempted and reproducibility information is still accessible through the function `get_reproducibility_summary()`.
The other standard chunk options can be used with `reproducibleR`-code chunks as usual. 
In particular, `eval=FALSE` suppressed execution of the code, `echo=FALSE` suppressed that the code is shown and so forth.

# Discussion

We introduced an R package that allows for R code chunks in R Markdown documents that are automatically tested for reproducibility. 
The output of the package is compatible with all pandoc output formats including the standard choices html, pdf, and docx. 
Templates can be used to customize the output and summary functions allow users to generate arbitrary reproducibility reports either within the document or as separate files.

### What could possible go wrong?

In the following, we briefly discuss a few situations, in which threats to reproducibility happened and explain how the package handles these situations by default:

- Error `In get_engine(options$engine) :  Unknown language engine 'reproducibleR'` This error is shown if a R Markdown file is knitted and the `reproducibleRchunks` package was not loaded. Fix this by adding the statement `library(reproducibleRchunks)` in the first code chunk of the R Markdown document.
  
- Original computations were executed and stored in the JSON data file. Later, a reproduction attempt is made using the exact same R Markdown file but on a different computer. This is a classic case of non-reproducibility, which often is due to changes in the software packages and the R version the computations in the R Markdown rely upon [@epskamp2019reproducibility, @peikert2021reproducible]. Note that the goal of this package is not to guarantee reproducibility but to allow for automated testing and reporting of reproducibility. To increase the chances of reproducibility in the first place, various solutions exist [@peikert2021reproducible, @renv2022, @chan2023rang, @nagraj2023pracpac] of which a primary aspect is a way to document and recreate the original computational environment.

- Some computations are executed and their results are stored in a JSON file as planned. Later on, someone changes the code in the R Markdown file, such that the results differ and a failure of reproduction is indicated. 

- `R` is not available anymore. As with every fashion, some day `R` will be superseded by a future programming language and/or there will be no port of `R` or the `reproducibleRchunks` package to a future computing platform. Then, the JSON format will likely still allow the possibility to read out the original computational results in that future programming language as the JSON format itself is open and transparent. 

### Rigor in software development

In developing this package, we adhere to three major aspects of rigor in scientific software development [@brandmaier2024commentary]. 
First, the package comes with a variety of formal tests (based on the `testthat` package, @testthat2011) to test correct functioning of the package.
Second, we provide documentation in form of this manuscript and an online documentation ([https://github.com/brandmaier/reproducibleRchunks](https://github.com/brandmaier/reproducibleRchunks)). 
Third, bug reports and feature requests can be submitted through our github project website. 

### Storing and publishing reproducibility information

We strongly discourage making reproducibility checks on complete raw data. 
This is mainly for two reasons.
First, there may arise unwanted data protection issues when raw data is (accidentally) stored, copied or published via the reproducibility JSON files.
Second, the JSON format is inefficient in terms of storage space required.
It is designed to be human-readable and conformable with versioning tools such as git.
It is not meant for storing large data and will blow up file size by a considerable factor (while at the same time losing numerical precision).
At the same time, ensuring that the data set identical to the one used in the original analysis was used, adding a check on the contents of a `data.frame`.

### Summary

Again, we emphasize that this package is not meant to ensure reproducibility but allows for automatically testing reproducibility.

Even though, a variety of approaches have been suggested to ensure reproducibility of computations in R Markdown documents [@peikert2021reproducible, @renv2022, @chan2023rang, @nagraj2023pracpac], we ourselves have encountered various situations, in which such approaches (including our own) failed. 
First and foremost, many approaches rely on further software packages such as Docker, which essentially provide virtual environments that run identical on different machines (or the same machine at different time points, e.g., before and after upgrade of R or some or all packages used).
On some machines, Docker may simply be not available, either because it is not (yet) available on a certain newly introduced hardware (e.g., when Apple switched to their own processors) or because a user does not have admin privileges on a machine.
Further some approaches rely on service providers such as the MRAN archive, which was unexpectedly terminated. 
These different Achilles' heels at least occasionally lead to situations, in which users will try to locally reproduce historic computational results.
With our package, the success of these reproductions becomes easily testable.

We hope this package helps increasing the visibility of potential non-reproducibility issues in code and that this eventually increases quality of our research assets.
\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
